{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: This is the Jupyter notebook to do the following things:\n",
    "\n",
    "1. Read slimmed PKU Tree files\n",
    "2. Store the raw MC histograms to pickle files\n",
    "\n",
    "kernel: HWW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import awkward as ak\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "import boost_histogram as bh\n",
    "from cycler import cycler\n",
    "import uproot\n",
    "import pickle as pkl\n",
    "import hist as hist2\n",
    "from typing import Dict, List\n",
    "from dataclasses import dataclass, field\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = \"2018\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = \".\"\n",
    "\n",
    "plot_dir = f\"{MAIN_DIR}/templates/15Jul2024\"\n",
    "os.makedirs(plot_dir, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read SlimmedTree files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the slimmedtree files using uproot\n",
    "\n",
    "#different year available here.\n",
    "# year = \"2016APV\"\n",
    "# year = \"2016\"\n",
    "# year = \"2017\"\n",
    "# year = \"2018\"\n",
    "# year = \"Full-Run2\"\n",
    "year = YEAR\n",
    "\n",
    "CustNanoData = {\n",
    "    'data'   : f\"/data/bond/lyazj/SlimmedTree/V0/{year}/Data/SlimmedTree_JetHT_*.root\",\n",
    "    'QCD'    : f\"/data/bond/lyazj/SlimmedTree/V0/{year}/MC/SlimmedTree_QCD.root\",\n",
    "    'Top'    : f\"/data/bond/lyazj/SlimmedTree/V0/{year}/MC/SlimmedTree_Top.root\",\n",
    "    'WJets'  : f\"/data/bond/lyazj/SlimmedTree/V0/{year}/MC/SlimmedTree_WJets.root\",\n",
    "    'Rest'   : f\"/data/bond/lyazj/SlimmedTree/V0/{year}/MC/SlimmedTree_Rest.root\",\n",
    "    'Signal' : f\"/data/bond/lyazj/SlimmedTree/V0/{year}/MC/SlimmedTree_Signal.root\",\n",
    "}\n",
    "\n",
    "files = { }\n",
    "for typefile in CustNanoData:\n",
    "    files[typefile] = uproot.lazy({CustNanoData[typefile]: \"PKUTree\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': <Array [{SF_unc: 0, ... PTj_V2_c: 425}] type='41897087 * {\"SF_unc\": float32, \"PT...'>,\n",
       " 'QCD': <Array [{Mj_jesAbsolute_yearDown_b: -99, ... ] type='38349513 * {\"Mj_jesAbsolute...'>,\n",
       " 'Top': <Array [{b_QCDbb: -99, ... SF_unc: 0.0375}] type='7727422 * {\"b_QCDbb\": float32,...'>,\n",
       " 'WJets': <Array [{LHEScaleWeight_8: 0.81, ... ] type='4819765 * {\"LHEScaleWeight_8\": floa...'>,\n",
       " 'Rest': <Array [{SF_unc: 0.0162, ... Phij_max: 0.835}] type='3121448 * {\"SF_unc\": float6...'>,\n",
       " 'Signal': <Array [{LHEScaleWeight_6: 0.98, ... ] type='4294 * {\"LHEScaleWeight_6\": float32...'>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add tagger of: data\n",
      "Add tagger of: QCD\n",
      "Add tagger of: Top\n",
      "Add tagger of: WJets\n",
      "Add tagger of: Rest\n",
      "Add tagger of: Signal\n"
     ]
    }
   ],
   "source": [
    "def add_tagger(events):\n",
    "    events[\"probQCD\"] = ak.zeros_like(events[\"a_Hbc\"])\n",
    "    qcd_list = [\n",
    "        \"a_QCDbb\",\n",
    "        \"a_QCDcc\",\n",
    "        \"a_QCDb\",\n",
    "        \"a_QCDc\",\n",
    "        \"a_QCDothers\",\n",
    "    ]\n",
    "    for score in qcd_list:\n",
    "        events[\"probQCD\"] = events[\"probQCD\"] + events[score]\n",
    "    events[\"HbcVSQCS\"] = (events[\"a_Hbc\"])/(events[\"a_Hbc\"] + 0.997032*events[\"probQCD\"] + 0.002968*events[\"a_Hcs\"])\n",
    "for k in files:\n",
    "    print(\"Add tagger of:\",k)\n",
    "    add_tagger(events=files[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some test about variables / output all the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Etaj',\n",
       " 'Etaj_2',\n",
       " 'Etaj_3',\n",
       " 'Etaj_V2_a',\n",
       " 'Etaj_V2_b',\n",
       " 'Etaj_V2_c',\n",
       " 'Etaj_max',\n",
       " 'Etaj_mid',\n",
       " 'Etaj_min',\n",
       " 'HT',\n",
       " 'HbcVSQCS',\n",
       " 'LHEScaleWeight_0',\n",
       " 'LHEScaleWeight_1',\n",
       " 'LHEScaleWeight_2',\n",
       " 'LHEScaleWeight_3',\n",
       " 'LHEScaleWeight_4',\n",
       " 'LHEScaleWeight_5',\n",
       " 'LHEScaleWeight_6',\n",
       " 'LHEScaleWeight_7',\n",
       " 'LHEScaleWeight_8',\n",
       " 'MET_et',\n",
       " 'MET_et_UEdown',\n",
       " 'MET_et_UEup',\n",
       " 'MET_phi',\n",
       " 'MET_phi_UEdown',\n",
       " 'MET_phi_UEup',\n",
       " 'MJJ',\n",
       " 'MJJJ',\n",
       " 'Mj',\n",
       " 'Mj_2',\n",
       " 'Mj_3',\n",
       " 'Mj_V2_a',\n",
       " 'Mj_V2_b',\n",
       " 'Mj_V2_c',\n",
       " 'Mj_corr_V2_a',\n",
       " 'Mj_corr_V2_b',\n",
       " 'Mj_corr_V2_c',\n",
       " 'Mj_jerDown_a',\n",
       " 'Mj_jerDown_b',\n",
       " 'Mj_jerDown_c',\n",
       " 'Mj_jerUp_a',\n",
       " 'Mj_jerUp_b',\n",
       " 'Mj_jerUp_c',\n",
       " 'Mj_jesAbsoluteDown_a',\n",
       " 'Mj_jesAbsoluteDown_b',\n",
       " 'Mj_jesAbsoluteDown_c',\n",
       " 'Mj_jesAbsoluteUp_a',\n",
       " 'Mj_jesAbsoluteUp_b',\n",
       " 'Mj_jesAbsoluteUp_c',\n",
       " 'Mj_jesAbsolute_yearDown_a',\n",
       " 'Mj_jesAbsolute_yearDown_b',\n",
       " 'Mj_jesAbsolute_yearDown_c',\n",
       " 'Mj_jesAbsolute_yearUp_a',\n",
       " 'Mj_jesAbsolute_yearUp_b',\n",
       " 'Mj_jesAbsolute_yearUp_c',\n",
       " 'Mj_jesBBEC1Down_a',\n",
       " 'Mj_jesBBEC1Down_b',\n",
       " 'Mj_jesBBEC1Down_c',\n",
       " 'Mj_jesBBEC1Up_a',\n",
       " 'Mj_jesBBEC1Up_b',\n",
       " 'Mj_jesBBEC1Up_c',\n",
       " 'Mj_jesBBEC1_yearDown_a',\n",
       " 'Mj_jesBBEC1_yearDown_b',\n",
       " 'Mj_jesBBEC1_yearDown_c',\n",
       " 'Mj_jesBBEC1_yearUp_a',\n",
       " 'Mj_jesBBEC1_yearUp_b',\n",
       " 'Mj_jesBBEC1_yearUp_c',\n",
       " 'Mj_jesEC2Down_a',\n",
       " 'Mj_jesEC2Down_b',\n",
       " 'Mj_jesEC2Down_c',\n",
       " 'Mj_jesEC2Up_a',\n",
       " 'Mj_jesEC2Up_b',\n",
       " 'Mj_jesEC2Up_c',\n",
       " 'Mj_jesEC2_yearDown_a',\n",
       " 'Mj_jesEC2_yearDown_b',\n",
       " 'Mj_jesEC2_yearDown_c',\n",
       " 'Mj_jesEC2_yearUp_a',\n",
       " 'Mj_jesEC2_yearUp_b',\n",
       " 'Mj_jesEC2_yearUp_c',\n",
       " 'Mj_jesFlavorQCDDown_a',\n",
       " 'Mj_jesFlavorQCDDown_b',\n",
       " 'Mj_jesFlavorQCDDown_c',\n",
       " 'Mj_jesFlavorQCDUp_a',\n",
       " 'Mj_jesFlavorQCDUp_b',\n",
       " 'Mj_jesFlavorQCDUp_c',\n",
       " 'Mj_jesHFDown_a',\n",
       " 'Mj_jesHFDown_b',\n",
       " 'Mj_jesHFDown_c',\n",
       " 'Mj_jesHFUp_a',\n",
       " 'Mj_jesHFUp_b',\n",
       " 'Mj_jesHFUp_c',\n",
       " 'Mj_jesHF_yearDown_a',\n",
       " 'Mj_jesHF_yearDown_b',\n",
       " 'Mj_jesHF_yearDown_c',\n",
       " 'Mj_jesHF_yearUp_a',\n",
       " 'Mj_jesHF_yearUp_b',\n",
       " 'Mj_jesHF_yearUp_c',\n",
       " 'Mj_jesRelativeBalDown_a',\n",
       " 'Mj_jesRelativeBalDown_b',\n",
       " 'Mj_jesRelativeBalDown_c',\n",
       " 'Mj_jesRelativeBalUp_a',\n",
       " 'Mj_jesRelativeBalUp_b',\n",
       " 'Mj_jesRelativeBalUp_c',\n",
       " 'Mj_jesRelativeSample_yearDown_a',\n",
       " 'Mj_jesRelativeSample_yearDown_b',\n",
       " 'Mj_jesRelativeSample_yearDown_c',\n",
       " 'Mj_jesRelativeSample_yearUp_a',\n",
       " 'Mj_jesRelativeSample_yearUp_b',\n",
       " 'Mj_jesRelativeSample_yearUp_c',\n",
       " 'Mj_jesTotalDown_a',\n",
       " 'Mj_jesTotalDown_b',\n",
       " 'Mj_jesTotalDown_c',\n",
       " 'Mj_jesTotalUp_a',\n",
       " 'Mj_jesTotalUp_b',\n",
       " 'Mj_jesTotalUp_c',\n",
       " 'Mj_jmrDown_a',\n",
       " 'Mj_jmrDown_b',\n",
       " 'Mj_jmrDown_c',\n",
       " 'Mj_jmrUp_a',\n",
       " 'Mj_jmrUp_b',\n",
       " 'Mj_jmrUp_c',\n",
       " 'Mj_jmsDown_a',\n",
       " 'Mj_jmsDown_b',\n",
       " 'Mj_jmsDown_c',\n",
       " 'Mj_jmsUp_a',\n",
       " 'Mj_jmsUp_b',\n",
       " 'Mj_jmsUp_c',\n",
       " 'Mj_max',\n",
       " 'Mj_mid',\n",
       " 'Mj_min',\n",
       " 'Nj4_ex',\n",
       " 'Nj4_in',\n",
       " 'Nj8',\n",
       " 'PSWeight_0',\n",
       " 'PSWeight_1',\n",
       " 'PSWeight_2',\n",
       " 'PSWeight_3',\n",
       " 'PTj',\n",
       " 'PTj_2',\n",
       " 'PTj_3',\n",
       " 'PTj_V2_a',\n",
       " 'PTj_V2_b',\n",
       " 'PTj_V2_c',\n",
       " 'PTj_max',\n",
       " 'PTj_mid',\n",
       " 'PTj_min',\n",
       " 'Phij',\n",
       " 'Phij_2',\n",
       " 'Phij_3',\n",
       " 'Phij_V2_a',\n",
       " 'Phij_V2_b',\n",
       " 'Phij_V2_c',\n",
       " 'Phij_max',\n",
       " 'Phij_mid',\n",
       " 'Phij_min',\n",
       " 'PrefireWeight',\n",
       " 'PrefireWeightDown',\n",
       " 'PrefireWeightUp',\n",
       " 'SF',\n",
       " 'SF_unc',\n",
       " 'ST',\n",
       " 'a_HWW_V2',\n",
       " 'a_HWWvsQCD_V2',\n",
       " 'a_Hbb',\n",
       " 'a_Hbc',\n",
       " 'a_Hcc',\n",
       " 'a_Hcs',\n",
       " 'a_Hqq',\n",
       " 'a_Hss',\n",
       " 'a_QCDb',\n",
       " 'a_QCDbb',\n",
       " 'a_QCDc',\n",
       " 'a_QCDcc',\n",
       " 'a_QCDothers',\n",
       " 'b_HWW_V2',\n",
       " 'b_HWWvsQCD_V2',\n",
       " 'b_Hbb',\n",
       " 'b_Hbc',\n",
       " 'b_Hcc',\n",
       " 'b_Hcs',\n",
       " 'b_Hqq',\n",
       " 'b_Hss',\n",
       " 'b_QCDb',\n",
       " 'b_QCDbb',\n",
       " 'b_QCDc',\n",
       " 'b_QCDcc',\n",
       " 'b_QCDothers',\n",
       " 'c_HWW_V2',\n",
       " 'c_HWWvsQCD_V2',\n",
       " 'c_Hbb',\n",
       " 'c_Hbc',\n",
       " 'c_Hcc',\n",
       " 'c_Hcs',\n",
       " 'c_Hqq',\n",
       " 'c_Hss',\n",
       " 'c_QCDb',\n",
       " 'c_QCDbb',\n",
       " 'c_QCDc',\n",
       " 'c_QCDcc',\n",
       " 'c_QCDothers',\n",
       " 'nb_l_deep_ex',\n",
       " 'nb_l_deep_in',\n",
       " 'nb_m_deep_ex',\n",
       " 'nb_m_deep_in',\n",
       " 'nb_t_deep_ex',\n",
       " 'nb_t_deep_in',\n",
       " 'probQCD',\n",
       " 'puWeight',\n",
       " 'puWeightDown',\n",
       " 'puWeightUp',\n",
       " 'weight']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(files[\"WJets\"].fields)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### plot setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "import boost_histogram as bh\n",
    "from cycler import cycler\n",
    "\n",
    "use_helvet = False ## true: use helvetica for plots, make sure the system have the font installed\n",
    "if use_helvet:\n",
    "    CMShelvet = hep.style.CMS\n",
    "    CMShelvet['font.sans-serif'] = ['Helvetica', 'Arial']\n",
    "    plt.style.use(CMShelvet)\n",
    "else:\n",
    "    plt.style.use(hep.style.CMS)\n",
    "\n",
    "def flow(hist: bh.Histogram, overflow: bool=True, underflow: bool=True):\n",
    "    h, var = hist.view(flow=(overflow | underflow)).value, hist.view(flow=(overflow | underflow)).variance\n",
    "    if overflow: \n",
    "        # h, var also include underflow bins but in plots usually no underflow data\n",
    "        # And we've filled None with -999, so we shouldn't show underflow data (mostly from filled None)\n",
    "        # You have to access the overflow and underflow bins data like below:\n",
    "        h[-2] += h[-1]; var[-2] += var[-1]\n",
    "    if underflow:\n",
    "        h[1] += h[0]; var[1] += var[0]\n",
    "    if overflow or underflow:\n",
    "        h, var = h[1:-1], var[1:-1]\n",
    "    return h, var\n",
    "    # Return the updated histogram and variance\n",
    "\n",
    "def error_bar(h, var, type='data'):\n",
    "    from scipy.interpolate import CubicSpline\n",
    "    if type == 'data':\n",
    "        number = h\n",
    "    elif type == 'mc':  # h = k*N, var = k^2*N, std = k*sqrt(N)\n",
    "        number = h**2 / var\n",
    "    else:\n",
    "        raise ValueError(\"type should be 'data' or 'mc'! \")\n",
    "    center = range(11) # Number: 0-10\n",
    "    up = np.array([1.84, 3.30, 4.64, 5.92, 7.16, 8.38, 9.58, 10.77, 11.95, 13.11, 14.27]) - center\n",
    "    down = center - np.array([0, 0.17, 0.71, 1.37, 2.09, 2.84, 3.62, 4.42, 5.23, 6.06, 6.89])\n",
    "    #cs means to create a CubicSpline object\n",
    "    cs_up = CubicSpline(x=center, y=up)\n",
    "    cs_down = CubicSpline(x=center, y=down)\n",
    "    \n",
    "    Garwood = (number>0)&(number<10)\n",
    "    poison_error_bar = np.sqrt(number)\n",
    "    up_error_bar = np.copy(poison_error_bar)\n",
    "    down_error_bar = np.copy(poison_error_bar)\n",
    "    up_error_bar[Garwood] = cs_up(number[Garwood])\n",
    "    down_error_bar[Garwood] = cs_down(number[Garwood])\n",
    "    if type == 'mc':\n",
    "        up_error_bar *= var/h\n",
    "        down_error_bar *= var/h\n",
    "    up_error_bar [up_error_bar < 0 ] = 0\n",
    "    down_error_bar [down_error_bar < 0 ] = 0\n",
    "    return np.array([down_error_bar, up_error_bar])\n",
    "\n",
    "\n",
    "# function to find the optimal region with S/sqrt(B)\n",
    "# not used so far\n",
    "def optimalcut(shist, bhist):\n",
    "    n_bins = len(shist)\n",
    "    best_lower = None\n",
    "    best_upper = None\n",
    "    best_s_sqrt_b = 0\n",
    "\n",
    "    for lower in range(n_bins):\n",
    "        for upper in range(lower+1, n_bins+1):\n",
    "            s = np.sum(shist[lower:upper])\n",
    "            b = np.sum(bhist[lower:upper])\n",
    "            s_sqrt_b = s / np.sqrt(b + 1)\n",
    "\n",
    "            if s_sqrt_b > best_s_sqrt_b:\n",
    "                best_lower = lower\n",
    "                best_upper = upper\n",
    "                best_s_sqrt_b = s_sqrt_b\n",
    "\n",
    "    return best_lower, best_upper, best_s_sqrt_b\n",
    "\n",
    "def optimalcut_oneside(shist, bhist, epsilon = 0.01):\n",
    "    '''\n",
    "    Given the signal histogram and background histogram, \n",
    "    show the one-side cut for the variable to get best s/sqrt(b).\n",
    "    Args:\n",
    "        shist:signal histogram\n",
    "        bhist:background histogram\n",
    "        epsilon(float): epsilon to avoid numerical errs \n",
    "    '''\n",
    "    n_bins = len(shist)\n",
    "    best_cut = 0\n",
    "    best_s_sqrt_b = 0\n",
    "\n",
    "    for cut in range(n_bins):\n",
    "        s = np.sum(shist[cut:])\n",
    "        b = np.sum(bhist[cut:])\n",
    "        s_sqrt_b = s / np.sqrt(b + epsilon)\n",
    "        if s_sqrt_b > best_s_sqrt_b:\n",
    "            best_cut = cut\n",
    "            best_s_sqrt_b = s_sqrt_b\n",
    "        \n",
    "    return best_cut, best_s_sqrt_b\n",
    "\n",
    "def optimalcut_mid_combine(shist1, shist2, bhist, epsilon = 1):\n",
    "    '''\n",
    "    Given the signal histogram and background histogram, \n",
    "    show the one-side cut for the variable to get best s/sqrt(b).\n",
    "    Args:\n",
    "        shist:signal histogram\n",
    "        bhist:background histogram\n",
    "        epsilon(float): epsilon to avoid numerical errs \n",
    "    '''\n",
    "    n_bins = len(shist1)\n",
    "    best_cut = 0\n",
    "    best_combined_sig_two_side = 0\n",
    "\n",
    "    for cut in range(n_bins):\n",
    "        s_right_side = np.sum(shist2[cut:])\n",
    "        b_right_side = np.sum(bhist[cut:])\n",
    "        s_left_side = np.sum(shist1[:cut])\n",
    "        b_left_side = np.sum(bhist[:cut])\n",
    "        s_sqrt_b_right_side = s_right_side / np.sqrt(b_right_side + epsilon)\n",
    "        s_sqrt_b_left_side = s_left_side / np.sqrt(b_left_side + epsilon)\n",
    "        combined_sig_two_side = np.sqrt((s_sqrt_b_right_side)**2 + (s_sqrt_b_left_side)**2)\n",
    "        if combined_sig_two_side > best_combined_sig_two_side:\n",
    "            best_cut = cut\n",
    "            best_combined_sig_two_side = combined_sig_two_side\n",
    "        \n",
    "    return best_cut, best_combined_sig_two_side\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define observable object variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ShapeVar:\n",
    "    \"\"\"Class to store attributes of a variable to make a histogram of.\n",
    "\n",
    "    Args:\n",
    "        var (str): variable name\n",
    "        label (str): variable label\n",
    "        bins (List[int]): bins\n",
    "        reg (bool, optional): Use a regular axis or variable binning. Defaults to True.\n",
    "        blind_window (List[int], optional): if blinding, set min and max values to set 0. Defaults to None.\n",
    "        significance_dir (str, optional): if plotting significance, which direction to plot it in.\n",
    "          See more in plotting.py:ratioHistPlot(). Options are [\"left\", \"right\", \"bin\"]. Defaults to \"right\".\n",
    "    \"\"\"\n",
    "\n",
    "    var: str = None\n",
    "    label: str = None\n",
    "    bins: List[int] = None\n",
    "    reg: bool = True #regular axis\n",
    "    blind_window: List[int] = None\n",
    "    significance_dir: str = \"right\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # create axis used for histogramming\n",
    "        if self.reg:\n",
    "            self.axis = hist2.axis.Regular(*self.bins, name=self.var, label=self.label)\n",
    "        else:\n",
    "            self.axis = hist2.axis.Variable(self.bins, name=self.var, label=self.label)\n",
    "\n",
    "@dataclass\n",
    "class Syst:\n",
    "    samples: list[str] = None\n",
    "    years: list[str] = field(default_factory=lambda: years)\n",
    "    label: str = None\n",
    "    \n",
    "def blindBins(h: hist2.Hist, blind_region: List, blind_sample: str = None, axis=0):\n",
    "    \"\"\"\n",
    "    Blind (i.e. zero) bins in histogram ``h``.\n",
    "    If ``blind_sample`` specified, only blind that sample, else blinds all.\n",
    "    \"\"\"\n",
    "    if axis > 0:\n",
    "        raise Exception(\"not implemented > 1D blinding yet\")\n",
    "\n",
    "    bins = h.axes[axis + 1].edges\n",
    "    lv = int(np.searchsorted(bins, blind_region[0], \"right\"))\n",
    "    rv = int(np.searchsorted(bins, blind_region[1], \"left\") + 1)\n",
    "\n",
    "    if blind_sample is not None:\n",
    "        data_key_index = np.where(np.array(list(h.axes[0])) == blind_sample)[0][0]\n",
    "        h.view(flow=True)[data_key_index][lv:rv].value = 0\n",
    "        h.view(flow=True)[data_key_index][lv:rv].variance = 0\n",
    "    else:\n",
    "        h.view(flow=True)[:, lv:rv].value = 0\n",
    "        h.view(flow=True)[:, lv:rv].variance = 0       \n",
    "shape_vars = [\n",
    "    ShapeVar(\n",
    "        \"MH_Reco\",\n",
    "        r\"Wcb candidate soft-drop mass [GeV]\",\n",
    "        [20, 30, 230],\n",
    "        reg=True,\n",
    "        blind_window=[50, 110],\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define samples we consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_keys = [\n",
    "    \"Signal\",\n",
    "]\n",
    "\n",
    "bkg_keys = [\n",
    "    \"Top\",\n",
    "    \"WJets\",\n",
    "    \"Rest\"\n",
    "]\n",
    "# no QCD here, since we will use data-driven for QCD\n",
    "mc_keys = sig_keys + bkg_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define weight shift list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [\"2016APV\", \"2016\", \"2017\", \"2018\"]\n",
    "\n",
    "weight_shifts = {\n",
    "    \"pileup\": Syst(samples=mc_keys, label=\"Pileup\"),\n",
    "    \"ISRPartonShower\": Syst(samples=mc_keys, label=\"ISR Parton Shower\"),\n",
    "    \"FSRPartonShower\": Syst(samples=mc_keys, label=\"FSR Parton Shower\"),\n",
    "    \"QCDscale\": Syst(samples=bkg_keys, label=\"QCDScale\"),\n",
    "    # \"PDFscale\": Syst(samples=bkg_keys, label=\"PDFscale\"),\n",
    "    \"L1Prefiring\": Syst(samples=mc_keys, label=\"L1Prefiring\"),\n",
    "    \"triggerEffSF_uncorrelated\" : Syst(samples=mc_keys, label=\"Trigger SF\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-organize weight information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down:\n",
      "  - Signal:\n",
      "      - pileup\n",
      "  - Top:\n",
      "      - pileup\n",
      "  - WJets:\n",
      "      - pileup\n",
      "  - Rest:\n",
      "      - pileup\n",
      "  - Signal:\n",
      "      - ISRPartonShower\n",
      "  - Top:\n",
      "      - ISRPartonShower\n",
      "  - WJets:\n",
      "      - ISRPartonShower\n",
      "  - Rest:\n",
      "      - ISRPartonShower\n",
      "  - Signal:\n",
      "      - FSRPartonShower\n",
      "  - Top:\n",
      "      - FSRPartonShower\n",
      "  - WJets:\n",
      "      - FSRPartonShower\n",
      "  - Rest:\n",
      "      - FSRPartonShower\n",
      "  - Top:\n",
      "      - QCDscale\n",
      "  - WJets:\n",
      "      - QCDscale\n",
      "  - Rest:\n",
      "      - QCDscale\n",
      "  - Signal:\n",
      "      - L1Prefiring\n",
      "  - Top:\n",
      "      - L1Prefiring\n",
      "  - WJets:\n",
      "      - L1Prefiring\n",
      "  - Rest:\n",
      "      - L1Prefiring\n",
      "  - Signal:\n",
      "      - triggerEffSF_uncorrelated\n",
      "  - Top:\n",
      "      - triggerEffSF_uncorrelated\n",
      "  - WJets:\n",
      "      - triggerEffSF_uncorrelated\n",
      "  - Rest:\n",
      "      - triggerEffSF_uncorrelated\n",
      "up:\n",
      "  - Signal:\n",
      "      - pileup\n",
      "  - Top:\n",
      "      - pileup\n",
      "  - WJets:\n",
      "      - pileup\n",
      "  - Rest:\n",
      "      - pileup\n",
      "  - Signal:\n",
      "      - ISRPartonShower\n",
      "  - Top:\n",
      "      - ISRPartonShower\n",
      "  - WJets:\n",
      "      - ISRPartonShower\n",
      "  - Rest:\n",
      "      - ISRPartonShower\n",
      "  - Signal:\n",
      "      - FSRPartonShower\n",
      "  - Top:\n",
      "      - FSRPartonShower\n",
      "  - WJets:\n",
      "      - FSRPartonShower\n",
      "  - Rest:\n",
      "      - FSRPartonShower\n",
      "  - Top:\n",
      "      - QCDscale\n",
      "  - WJets:\n",
      "      - QCDscale\n",
      "  - Rest:\n",
      "      - QCDscale\n",
      "  - Signal:\n",
      "      - L1Prefiring\n",
      "  - Top:\n",
      "      - L1Prefiring\n",
      "  - WJets:\n",
      "      - L1Prefiring\n",
      "  - Rest:\n",
      "      - L1Prefiring\n",
      "  - Signal:\n",
      "      - triggerEffSF_uncorrelated\n",
      "  - Top:\n",
      "      - triggerEffSF_uncorrelated\n",
      "  - WJets:\n",
      "      - triggerEffSF_uncorrelated\n",
      "  - Rest:\n",
      "      - triggerEffSF_uncorrelated\n"
     ]
    }
   ],
   "source": [
    "samples = list(['data','QCD','Top','WJets','Rest','Signal']) #all samples we considered\n",
    "years = [\"2016APV\", \"2016\", \"2017\", \"2018\"]#not used here finally\n",
    "year_to_run = YEAR #not used here finally\n",
    "for shift in [\"down\", \"up\"]:\n",
    "    print(shift, end=':\\n')\n",
    "    for wshift, wsyst in weight_shifts.items():\n",
    "        for wsample in wsyst.samples:\n",
    "            print(\"  -\", wsample, end=':\\n')\n",
    "            if wsample in samples:\n",
    "                if wshift == \"pileup\" :\n",
    "                    print(\"      -\", wshift)\n",
    "                    if shift == \"up\":\n",
    "                        files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * (files[wsample][\"puWeightUp\"] / files[wsample][\"puWeight\"])\n",
    "                    if shift == \"down\":\n",
    "                        files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * (files[wsample][\"puWeightDown\"] / files[wsample][\"puWeight\"])\n",
    "                if wshift == \"ISRPartonShower\" :\n",
    "                    print(\"      -\", wshift)\n",
    "                    if shift == \"up\":\n",
    "                        files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * files[wsample][\"PSWeight_0\"]\n",
    "                    if shift == \"down\":\n",
    "                        files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * files[wsample][\"PSWeight_2\"] \n",
    "                if wshift == \"FSRPartonShower\" :\n",
    "                    print(\"      -\", wshift)\n",
    "                    if shift == \"up\":\n",
    "                        files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * files[wsample][\"PSWeight_1\"]\n",
    "                    if shift == \"down\":\n",
    "                        files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * files[wsample][\"PSWeight_3\"] \n",
    "                if wshift == \"QCDscale\" :\n",
    "                    print(\"      -\", wshift)\n",
    "                    if shift == \"up\":\n",
    "                        files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * files[wsample][\"LHEScaleWeight_8\"]\n",
    "                    if shift == \"down\":\n",
    "                        files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * files[wsample][\"LHEScaleWeight_0\"] \n",
    "                if wshift == \"L1Prefiring\" :\n",
    "                    #note that no L1Prefiring for 2018, so simply set to 1\n",
    "                    print(\"      -\", wshift)\n",
    "                    if shift == \"up\":\n",
    "                        if year_to_run == \"2018\":\n",
    "                            files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"]\n",
    "                        else:\n",
    "                            files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * (files[wsample][\"PrefireWeightUp\"] / files[wsample][\"PrefireWeight\"])\n",
    "                    if shift == \"down\":\n",
    "                        if year_to_run == \"2018\":\n",
    "                            files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"]\n",
    "                        else:\n",
    "                            files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * (files[wsample][\"PrefireWeightDown\"] / files[wsample][\"PrefireWeight\"])\n",
    "                if wshift == \"triggerEffSF_uncorrelated\" :\n",
    "                    print(\"      -\", wshift)\n",
    "                    if shift == \"up\":\n",
    "                        files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * (1 + files[wsample][\"SF_unc\"])\n",
    "                    if shift == \"down\":\n",
    "                        files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * (1 - files[wsample][\"SF_unc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 0x7f1cf0811b20\n",
      "QCD 0x7f1cf09992e0\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n"
     ]
    }
   ],
   "source": [
    "for item in files:\n",
    "    print(item, hex(id(files[item])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define variation shift list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "jecs = {\n",
    "    \"JES\"     : \"JES_jes\",\n",
    "    \"JER\"     : \"JER\",\n",
    "    \"JMS\"     : \"JMS\",\n",
    "    \"JMR\"     : \"JMR\",\n",
    "    \"Absolute\": \"split\",\n",
    "    \"Absolute_year\": \"split\",\n",
    "    \"BBEC1\": \"split\",\n",
    "    \"BBEC1_year\": \"split\",\n",
    "    \"EC2\": \"split\",\n",
    "    \"EC2_year\": \"split\",\n",
    "    \"FlavorQCD\": \"split\",\n",
    "    \"HF\": \"split\",\n",
    "    \"HF_year\": \"split\",\n",
    "    \"RelativeBal\": \"split\",\n",
    "    \"RelativeSample_year\": \"split\",\n",
    "}\n",
    "\n",
    "# uncluste = {\n",
    "#     \"UE\": \"unclusteredEnergy\",\n",
    "# }\n",
    "# no need to use MET\n",
    "\n",
    "# pdf = {\n",
    "#     \"pdfscale\": \"pdfscale\",\n",
    "# }\n",
    "\n",
    "jec_shifts = {}\n",
    "for key in jecs:\n",
    "    for shift in [\"up\", \"down\"]:\n",
    "        if key == \"JES\": \n",
    "            if shift == \"up\"   : jec_shifts[f\"{key}_{shift}\"] = \"Mj_jesTotalUp_a\"\n",
    "            if shift == \"down\" : jec_shifts[f\"{key}_{shift}\"] = \"Mj_jesTotalDown_a\"\n",
    "        elif key == \"JER\": \n",
    "            if shift == \"up\"   : jec_shifts[f\"{key}_{shift}\"] = \"Mj_jerUp_a\"\n",
    "            if shift == \"down\" : jec_shifts[f\"{key}_{shift}\"] = \"Mj_jerDown_a\"\n",
    "        elif key == \"JMS\": \n",
    "            if shift == \"up\"   : jec_shifts[f\"{key}_{shift}\"] = \"Mj_jmsUp_a\"\n",
    "            if shift == \"down\" : jec_shifts[f\"{key}_{shift}\"] = \"Mj_jmsDown_a\"\n",
    "        elif key == \"JMR\": \n",
    "            if shift == \"up\"   : jec_shifts[f\"{key}_{shift}\"] = \"Mj_jmrUp_a\"\n",
    "            if shift == \"down\" : jec_shifts[f\"{key}_{shift}\"] = \"Mj_jmrDown_a\"\n",
    "        else: #split JES\n",
    "            if shift == \"up\"   : jec_shifts[f\"{key}_{shift}\"] = f\"Mj_jes{key}Up_a\"\n",
    "            if shift == \"down\" : jec_shifts[f\"{key}_{shift}\"] = f\"Mj_jes{key}Down_a\"\n",
    "\n",
    "# ue_shifts = {}\n",
    "# for key in uncluste:\n",
    "#     for shift in [\"up\", \"down\"]:\n",
    "#         if shift == \"up\"   : ue_shifts[f\"{key}_{shift}\"] = \"MH_Reco_UE_up\"\n",
    "#         if shift == \"down\" : ue_shifts[f\"{key}_{shift}\"] = \"MH_Reco_UE_down\"\n",
    "        \n",
    "# pdf_shifts = {}\n",
    "# #store nominal value first, and then the up/down variation\n",
    "# for key in pdf:\n",
    "#     for shift in [\"up\", \"down\"]:\n",
    "#         if shift == \"up\"   : pdf_shifts[f\"{key}_{shift}\"] = \"Mj_corr_V2_a\"\n",
    "#         if shift == \"down\" : pdf_shifts[f\"{key}_{shift}\"] = \"Mj_corr_V2_a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'JES_up': 'Mj_jesTotalUp_a',\n",
       " 'JES_down': 'Mj_jesTotalDown_a',\n",
       " 'JER_up': 'Mj_jerUp_a',\n",
       " 'JER_down': 'Mj_jerDown_a',\n",
       " 'JMS_up': 'Mj_jmsUp_a',\n",
       " 'JMS_down': 'Mj_jmsDown_a',\n",
       " 'JMR_up': 'Mj_jmrUp_a',\n",
       " 'JMR_down': 'Mj_jmrDown_a',\n",
       " 'Absolute_up': 'Mj_jesAbsoluteUp_a',\n",
       " 'Absolute_down': 'Mj_jesAbsoluteDown_a',\n",
       " 'Absolute_year_up': 'Mj_jesAbsolute_yearUp_a',\n",
       " 'Absolute_year_down': 'Mj_jesAbsolute_yearDown_a',\n",
       " 'BBEC1_up': 'Mj_jesBBEC1Up_a',\n",
       " 'BBEC1_down': 'Mj_jesBBEC1Down_a',\n",
       " 'BBEC1_year_up': 'Mj_jesBBEC1_yearUp_a',\n",
       " 'BBEC1_year_down': 'Mj_jesBBEC1_yearDown_a',\n",
       " 'EC2_up': 'Mj_jesEC2Up_a',\n",
       " 'EC2_down': 'Mj_jesEC2Down_a',\n",
       " 'EC2_year_up': 'Mj_jesEC2_yearUp_a',\n",
       " 'EC2_year_down': 'Mj_jesEC2_yearDown_a',\n",
       " 'FlavorQCD_up': 'Mj_jesFlavorQCDUp_a',\n",
       " 'FlavorQCD_down': 'Mj_jesFlavorQCDDown_a',\n",
       " 'HF_up': 'Mj_jesHFUp_a',\n",
       " 'HF_down': 'Mj_jesHFDown_a',\n",
       " 'HF_year_up': 'Mj_jesHF_yearUp_a',\n",
       " 'HF_year_down': 'Mj_jesHF_yearDown_a',\n",
       " 'RelativeBal_up': 'Mj_jesRelativeBalUp_a',\n",
       " 'RelativeBal_down': 'Mj_jesRelativeBalDown_a',\n",
       " 'RelativeSample_year_up': 'Mj_jesRelativeSample_yearUp_a',\n",
       " 'RelativeSample_year_down': 'Mj_jesRelativeSample_yearDown_a'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jec_shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CUT(aka. regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUT = {\n",
    "    \"SR1\" : {k:  (files[k][\"nb_t_deep_ex\"] == 0) & (files[k][\"HbcVSQCS\"] >= 0.998) for k in files},\n",
    "    \"SR2\" : {k:  (files[k][\"nb_t_deep_ex\"] == 0) & (files[k][\"HbcVSQCS\"] >= 0.98) & (files[k][\"HbcVSQCS\"] <= 0.998) for k in files},\n",
    "    \"SR3\" : {k:  (files[k][\"nb_t_deep_ex\"] == 0) & (files[k][\"HbcVSQCS\"] >= 0.9) & (files[k][\"HbcVSQCS\"] <= 0.98) for k in files},\n",
    "    \"CR\"  : {k:  (files[k][\"nb_t_deep_ex\"] == 0) & (files[k][\"HbcVSQCS\"] < 0.9)  for k in files},\n",
    "    \"PS\"  : {k:  (files[k][\"nb_t_deep_ex\"] >= 0)  for k in files},\n",
    "}\n",
    "# CR: tagger < 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files_cut = {typefile : {region : {} for region in CUT.keys()} for typefile in CustNanoData if (typefile != \"QCD\" and typefile != \"data\")}\n",
    "# for typefile in files_cut:\n",
    "#     if typefile != \"PLACE_HOLDER\":continue\n",
    "#     print(\"file\",typefile)\n",
    "#     for region in CUT.keys():\n",
    "#         print(\"region\",region)\n",
    "#         # files[typefile] = uproot.lazy({CustNanoData[typefile]: \"PKUTree\" })\n",
    "#         files_cut[typefile] = files[typefile][CUT[region][typefile]]\n",
    "#         if region.startswith(\"SR\"):\n",
    "#             continue\n",
    "#             keep_keys = {\n",
    "#                 \"MH_Reco\" : files_cut[typefile][\"MH_Reco\"][:1000000],\n",
    "#                 \"weight\"  : files_cut[typefile][\"weight\"][:1000000],\n",
    "#                 \"LHEPdfWeight\" : files_cut[typefile][\"LHEPdfWeight\"][:1000000],\n",
    "#             }\n",
    "#         else:\n",
    "#             keep_keys = {\n",
    "#                 \"MH_Reco\" : files_cut[typefile][\"MH_Reco\"][::50],\n",
    "#                 \"weight\"  : files_cut[typefile][\"weight\"][::50],\n",
    "#                 \"LHEPdfWeight\" : files_cut[typefile][\"LHEPdfWeight\"][::50],\n",
    "#             }\n",
    "        \n",
    "#         # array_dict = {name : files_cut[typefile].array(library=\"ak\") for name in keep_keys}\n",
    "#         tmp_file_name = os.path.normpath(f\"./root/{typefile}_{region}.root\")\n",
    "#         with uproot.recreate(tmp_file_name) as newfile:\n",
    "#             newfile[\"PKUTree\"] = keep_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define PDF variation individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_unc(events,x_min = 50, x_max = 250, nbins = 20, up_down = \"up\", max_run = 100000, extra_weight = 1.0):\n",
    "    '''\n",
    "    input: events\n",
    "    output: up, down variation histogram(both value and variance)\n",
    "    get pdf variation and save it as a weight unc in the end\n",
    "    since the computation is too large so we use a max_run cut\n",
    "    '''\n",
    "    histogram = np.zeros((len(events[\"LHEPdfWeight\"][0]), nbins))\n",
    "    num_weights = len(events[\"LHEPdfWeight\"][0])\n",
    "    # num_weights = 20\n",
    "    hist_pdf_i = bh.Histogram(bh.axis.Regular(nbins, x_min, x_max), storage=bh.storage.Weight())\n",
    "    # events_tmp = ak.copy(events)\n",
    "    # Loop over each weight\n",
    "    for i in range(0, num_weights):\n",
    "        print(\"now pdf\",i)\n",
    "        print(len(events[\"LHEPdfWeight\"]))\n",
    "        hist_pdf_i.reset()\n",
    "        hist_pdf_i.fill(events[\"MH_Reco\"][0:max_run],weight=events[\"weight\"][0:max_run] * events[\"LHEPdfWeight\"][0:max_run][:, i] * extra_weight)\n",
    "        histogram[i, :] = hist_pdf_i.view(flow=False).value\n",
    "    # Calculate histogram errors\n",
    "    histogram_stderr = np.sqrt(np.sum((histogram[1:] - histogram[0])**2, axis=0)) \n",
    "    print(\"nominal value:\",histogram[0])\n",
    "    # Calculate up and down variations\n",
    "    if up_down == \"up\":\n",
    "        variation_hist = histogram[0] + histogram_stderr\n",
    "        print(\"up\",variation_hist)\n",
    "    elif up_down == \"down\":\n",
    "        variation_hist = histogram[0] - histogram_stderr\n",
    "        print(\"down\",variation_hist)\n",
    "    else: \n",
    "        raise ValueError(\"wrong variation\")\n",
    "    # print(variation_hist)\n",
    "    return variation_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in files:\n",
    "    # note that QCD and Data don't have such variation\n",
    "    if k == \"data\" or k == \"QCD\": continue\n",
    "    if k != \"TT\" : continue\n",
    "    print(\"Add pdf of:\",k)\n",
    "    print(files[k])\n",
    "    # get_pdf_unc(events=files[k][CUT[\"SR1a\"][k]])\n",
    "    # get_pdf_unc(events=uproot.lazy({f\"./root/{year}_{k}_CR1.root\": \"PKUTree\" }))\n",
    "    # get_pdf_unc(events = uproot.lazy({f\"./root/{k}_CR1.root\": \"PKUTree\" }),up_down = \"up\", extra_weight = 1.0, max_run = 100000)\n",
    "    # get_pdf_unc(events=uproot.lazy({f\"./root/{k}_SR1b.root\": \"PKUTree\" }))\n",
    "    # get_pdf_unc(events=uproot.lazy({f\"./root/{k}_SR2a.root\": \"PKUTree\" }))\n",
    "    # get_pdf_unc(events=uproot.lazy({f\"./root/{k}_SR2b.root\": \"PKUTree\" }))\n",
    "    # get_pdf_unc(events=uproot.lazy({f\"./root/{k}_CR1.root\": \"PKUTree\" }))\n",
    "    # get_pdf_unc(events=uproot.lazy({f\"./root/{k}_CR2.root\": \"PKUTree\" }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save hist templates to pkl files\n",
    "\n",
    "We note here that no particular operation is needed for QCD, since we only need raw QCD MC ratio as initial tranfer factor in the actual QCD prediction, and rhalphabet method will use (data - other bkg) in fail(control) region and perform simultaneous fit with pass(signal) region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pkl(files, path_str = plot_dir, template_file = \"templates\",year_to_run = \"2018\"):\n",
    "    \n",
    "    templates = {} #empty dict to store the templates file\n",
    "    regions = [\"SR1\",\"SR2\",\"SR3\",\"CR\"] #signal regions or control regions\n",
    "    years = [\"2016APV\", \"2016\", \"2017\", \"2018\"]\n",
    "    samples = list(['data','QCD','Top','WJets','Rest','Signal']) #all samples we considered\n",
    "    print(\"now running year:\",year_to_run)\n",
    "    #initialize weight based variation samples\n",
    "    hist_samples = []\n",
    "    for shift in [\"down\", \"up\"]:\n",
    "        for wshift, wsyst in weight_shifts.items():\n",
    "            for wsample in wsyst.samples:\n",
    "                hist_samples.append(f\"{wsample}_{wshift}_{shift}\")\n",
    "    \n",
    "    hist_samples += samples\n",
    "    #fill templates for different regions\n",
    "    \n",
    "    for region in regions:\n",
    "        print(\"now processing:\",region)\n",
    "        templates[region] = hist2.Hist(\n",
    "            hist2.axis.StrCategory(hist_samples, name=\"Sample\"),\n",
    "            *[shape_var.axis for shape_var in shape_vars],\n",
    "            storage=\"weight\",\n",
    "            ) #initialize a hist object\n",
    "        \n",
    "        #add center value templates first\n",
    "        for sample in samples:\n",
    "            \n",
    "            #if region is signal region and sample is signal samples, we multiply by lund plane sf\n",
    "            weight_to_add = 1.0\n",
    "            \n",
    "            data = files[sample][CUT[region][sample]]\n",
    "            templates[region].fill(\n",
    "                                Sample=sample,\n",
    "                                MH_Reco=data[\"Mj_corr_V2_a\"],\n",
    "                                weight=data[\"weight\"] * weight_to_add,\n",
    "                            )\n",
    "            if sample == \"data\": \n",
    "                if (region.startswith(\"SR\")):\n",
    "                    # blind signal mass windows in pass region in data even for not \"Blinded\" region\n",
    "                    # print(\"blind data of \",region)\n",
    "                    for i, shape_var in enumerate(shape_vars):\n",
    "                        if shape_var.blind_window is not None:\n",
    "                            blindBins(templates[region], shape_var.blind_window, \"data\", axis=i)\n",
    "                            \n",
    "\n",
    "\n",
    "        #add weight based variation for each sample            \n",
    "        for shift in [\"down\", \"up\"]:\n",
    "            for wshift, wsyst in weight_shifts.items():\n",
    "                for wsample in wsyst.samples:\n",
    "                    if wsample in samples:\n",
    "                        data = files[wsample][CUT[region][wsample]]\n",
    "                        print(wsample, hex(id(files[wsample])))\n",
    "                        weight_to_add = 1.0\n",
    "                        templates[region].fill(\n",
    "                            Sample=wsample + f\"_{wshift}_{shift}\",\n",
    "                            MH_Reco=data[\"Mj_corr_V2_a\"],\n",
    "                            weight=data[f\"{wshift}_{shift}\"] * weight_to_add,\n",
    "                        )\n",
    "                        \n",
    "        #add shift variation for each sample\n",
    "        #1.initialize hist info\n",
    "        for wshift, wsyst in jec_shifts.items():\n",
    "            # split the JES/JER uncertainties according to year, i.e., one variation for each era\n",
    "            templates[f\"{region}_{wshift}\"] = hist2.Hist(\n",
    "            hist2.axis.StrCategory(samples, name=\"Sample\"),\n",
    "            *[shape_var.axis for shape_var in shape_vars],\n",
    "            storage=\"weight\",\n",
    "            ) #initialize a hist object\n",
    "        # for wshift, wsyst in pdf_shifts.items():\n",
    "        #     templates[f\"{region}_{wshift}\"] = hist2.Hist(\n",
    "        #     hist2.axis.StrCategory(samples, name=\"Sample\"),\n",
    "        #     *[shape_var.axis for shape_var in shape_vars],\n",
    "        #     storage=\"weight\",\n",
    "        #     ) #initialize a hist object                \n",
    "        #2.fill the hist\n",
    "        for sample in mc_keys:\n",
    "            data = files[sample][CUT[region][sample]] \n",
    "            #JECS\n",
    "            for wshift, wsyst in jec_shifts.items():\n",
    "                weight_to_add = 1.0\n",
    "                #assign same variation as center value for other years\n",
    "                templates[f\"{region}_{wshift}\"].fill(\n",
    "                        Sample=sample,\n",
    "                        MH_Reco=data[wsyst],\n",
    "                        weight=data[\"weight\"] * weight_to_add,\n",
    "                    )                                    \n",
    "            #pdf variation\n",
    "            # for wshift, wsyst in pdf_shifts.items():\n",
    "            #     weight_to_add = 1.0\n",
    "            #     templates[f\"{region}_{wshift}\"].fill(\n",
    "            #                     Sample=sample,pileup_down\n",
    "            #                     MH_Reco=data[wsyst], #just nominal MET recovery mass\n",
    "            #                     weight=data[\"weight\"] * weight_to_add,\n",
    "            #     )\n",
    "            #     #but we need to change the up/down variation\n",
    "            #     key_index = np.where(np.array(list(templates[f\"{region}_{wshift}\"].axes[0])) == sample)[0][0]\n",
    "            #     print(\"now processing pdf variation\", sample)\n",
    "            #     #make following faster\n",
    "            #     # data_tmp = ak.to_pandas(data)\n",
    "            #     if \"up\" in wshift:\n",
    "            #         #only apply change to SR but not CR!\n",
    "            #         if region.startswith(\"SR\"):\n",
    "            #         # get_pdf_unc(events = data_tmp,up_down = \"up\", extra_weight = weight_to_add, max_run = 100000)\n",
    "            #             templates[f\"{region}_{wshift}\"].view(flow = False)[key_index][:].value = get_pdf_unc(events = uproot.lazy({f\"./root/{year}_{sample}_{region}.root\": \"PKUTree\" }),up_down = \"up\", extra_weight = weight_to_add, max_run = 100000)\n",
    "            #     elif \"down\" in wshift:\n",
    "            #         if region.startswith(\"SR\"):\n",
    "            #         # get_pdf_unc(events = data_tmp,up_down = \"down\", extra_weight = weight_to_add, max_run = 100000)\n",
    "            #             templates[f\"{region}_{wshift}\"].view(flow = False)[key_index][:].value = get_pdf_unc(events = uproot.lazy({f\"./root/{year}_{sample}_{region}.root\": \"PKUTree\" }),up_down = \"down\", extra_weight = weight_to_add, max_run = 100000)\n",
    "                # print(\"now done processing pdf variation\",sample)\n",
    "        \n",
    "        #extra process for QCD and data, is this really needed?\n",
    "        # for sample in [\"QCD\",\"data\"]: # actually no need to run QCD as not used at all\n",
    "        # for sample in [\"data\"]:\n",
    "        #     #QCD and data doesn't have any j-shift nor weight based variation\n",
    "        #     print(\"now processing(QCD/data):\",sample)\n",
    "        #     #JEC variation\n",
    "        #     for wshift, wsyst in jec_shifts.items():\n",
    "        #         data = files[sample][CUT[region][sample]] \n",
    "        #         #always assign value with `MH_Reco` variable\n",
    "        #         templates[f\"{region}_{wshift}\"].fill(\n",
    "        #                 Sample=sample,\n",
    "        #                 MH_Reco=data[\"Mj_corr_V2_a\"],\n",
    "        #                 weight=data[\"weight\"],\n",
    "        #             )\n",
    "                \n",
    "        #         #do blind procedure\n",
    "        #         if sample == \"data\" and  (region.startswith(\"SR\")):\n",
    "        #             for i, shape_var in enumerate(shape_vars):\n",
    "        #                 if shape_var.blind_window is not None:\n",
    "        #                     blindBins(templates[f\"{region}_{wshift}\"], shape_var.blind_window, \"data\", axis=i)\n",
    "\n",
    "            #pdf variation\n",
    "            # for wshift, wsyst in pdf_shifts.items():\n",
    "            #     data = files[sample][CUT[region][sample]] \n",
    "            #     #always assign value with `MH_Reco` variable\n",
    "            #     templates[f\"{region}_{wshift}\"].fill(\n",
    "            #             Sample=sample,\n",
    "            #             MH_Reco=data[\"MH_Reco\"],\n",
    "            #             weight=data[\"weight\"],\n",
    "            #         )           \n",
    "            #     #do blind procedure\n",
    "            #     if sample == \"data\" and (region.endswith(\"a\") or region.endswith(\"b\")):\n",
    "            #         for i, shape_var in enumerate(shape_vars):\n",
    "            #             if shape_var.blind_window is not None:\n",
    "            #                 blindBins(templates[f\"{region}_{wshift}\"], shape_var.blind_window, \"data\", axis=i)\n",
    "                        \n",
    "        print(\"done fill template \",region)        \n",
    "    \n",
    "    #Creates blinded copies of each region's templates and saves a pickle of the templates\n",
    "    blind_window = shape_vars[0].blind_window\n",
    "    for label, template in list(templates.items()):\n",
    "        blinded_template = deepcopy(template)\n",
    "        blindBins(blinded_template, blind_window)\n",
    "        templates[f\"{label}Blinded\"] = blinded_template\n",
    "    \n",
    "    #save files\n",
    "    with open(f\"{path_str}/{template_file}_{year_to_run}.pkl\", \"wb\") as fp:\n",
    "        pkl.dump(templates, fp) # dump the templates of each region in a pkl file\n",
    "        print(\"Saved templates to\", f\"{template_file}_{year_to_run}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now running year: 2018\n",
      "now processing: SR1\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "done fill template  SR1\n",
      "now processing: SR2\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "done fill template  SR2\n",
      "now processing: SR3\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "done fill template  SR3\n",
      "now processing: CR\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n",
      "Signal 0x7f13c8f91a30\n",
      "Top 0x7f15cd9e2a30\n",
      "WJets 0x7f15ce252520\n",
      "Rest 0x7f1d75ea4250\n"
     ]
    }
   ],
   "source": [
    "save_pkl(files = files, year_to_run = YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLACE_HOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test about the templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{plot_dir}/templates_2018.pkl\",\"rb\") as f:\n",
    "    hists_template1 = pkl.load(f)\n",
    "hists_template1[\"SR1a\"][\"ggF\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template1[\"SR2a\"][\"ggF_pileup_up\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template1[\"SR2a\"][\"ggF\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template1[\"SR2a\"][\"ggF_pileup_down\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template1[\"SR1a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template1[\"SR1a_JES_up_2018\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template1[\"SR1a\"][\"TT_FSRPartonShower_up\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template1[\"SR1a\"][\"TT\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template1[\"SR1a\"][\"TT_FSRPartonShower_down\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_template1 = hists_template1[\"SR1a\"][\"QCD\", :]\n",
    "err = sample_template1.variances()\n",
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i , axis in enumerate(hists_template1[\"SR1a\"].axes[1:]):\n",
    "    print(i, axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some test of 1l side analysis for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/home/pku/zhaoyz/Higgs/HHbbVV/src/HHbbVV/postprocessing/templates/23Jun14/2018_templates.pkl\",\"rb\") as f:\n",
    "with open(\"/ospool/cms-user/yuzhe/BoostedHWW/prediction/boostedhiggs/combine/templates/v4/hists_templates_Run2.pkl\",\"rb\") as f:\n",
    "    hists_template2 = pkl.load(f)\n",
    "# hists_template[\"pass\"][\"QCD\",:]  \n",
    "# hists_template[\"pass\"][\"QCD\",:].sum().value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some test of HHbbVV analysis for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/home/pku/zhaoyz/Higgs/HHbbVV/src/HHbbVV/postprocessing/templates/23Jun14/2018_templates.pkl\",\"rb\") as f:\n",
    "with open(\"/ospool/cms-user/yuzhe/BoostedHWW/prediction/HHbbVV/src/HHbbVV/postprocessing/templates/24Mar15UpdateData/2018_templates.pkl\",\"rb\") as f:\n",
    "    hists_template2 = pkl.load(f)\n",
    "# hists_template[\"pass\"][\"QCD\",:]  \n",
    "# hists_template[\"pass\"][\"QCD\",:].sum().value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2[\"pass\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2[\"pass_JES_up\"][\"ST\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hist(\n",
    "  StrCategory(['HHbbVV', 'ggHH_kl_2p45_kt_1_HHbbVV', 'ggHH_kl_5_kt_1_HHbbVV', 'ggHH_kl_0_kt_1_HHbbVV', 'VBFHHbbVV', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV', 'QCD', 'Top', 'W+Jets', 'Z+Jets', 'Diboson', 'ggFHbb', 'VBFHbb', 'ZHbb', 'WHbb', 'ggZHbb', 'ttHbb', 'HWW', 'Data', 'HHbbVV_txbb_down', 'ggHH_kl_2p45_kt_1_HHbbVV_txbb_down', 'ggHH_kl_5_kt_1_HHbbVV_txbb_down', 'ggHH_kl_0_kt_1_HHbbVV_txbb_down', 'VBFHHbbVV_txbb_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_txbb_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_txbb_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_txbb_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_txbb_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_txbb_down', 'HHbbVV_pileup_down', 'ggHH_kl_2p45_kt_1_HHbbVV_pileup_down', 'ggHH_kl_5_kt_1_HHbbVV_pileup_down', 'ggHH_kl_0_kt_1_HHbbVV_pileup_down', 'VBFHHbbVV_pileup_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_pileup_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_pileup_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_pileup_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_pileup_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_pileup_down', 'TT_pileup_down', 'ST_pileup_down', 'W+Jets_pileup_down', 'Z+Jets_pileup_down', 'HHbbVV_pileupID_down', 'ggHH_kl_2p45_kt_1_HHbbVV_pileupID_down', 'ggHH_kl_5_kt_1_HHbbVV_pileupID_down', 'ggHH_kl_0_kt_1_HHbbVV_pileupID_down', 'VBFHHbbVV_pileupID_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_pileupID_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_pileupID_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_pileupID_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_pileupID_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_pileupID_down', 'TT_pileupID_down', 'ST_pileupID_down', 'W+Jets_pileupID_down', 'Z+Jets_pileupID_down', 'HHbbVV_ISRPartonShower_down', 'ggHH_kl_2p45_kt_1_HHbbVV_ISRPartonShower_down', 'ggHH_kl_5_kt_1_HHbbVV_ISRPartonShower_down', 'ggHH_kl_0_kt_1_HHbbVV_ISRPartonShower_down', 'VBFHHbbVV_ISRPartonShower_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_ISRPartonShower_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_ISRPartonShower_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_ISRPartonShower_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_ISRPartonShower_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_ISRPartonShower_down', 'TT_ISRPartonShower_down', 'ST_ISRPartonShower_down', 'W+Jets_ISRPartonShower_down', 'Z+Jets_ISRPartonShower_down', 'HHbbVV_FSRPartonShower_down', 'ggHH_kl_2p45_kt_1_HHbbVV_FSRPartonShower_down', 'ggHH_kl_5_kt_1_HHbbVV_FSRPartonShower_down', 'ggHH_kl_0_kt_1_HHbbVV_FSRPartonShower_down', 'VBFHHbbVV_FSRPartonShower_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_FSRPartonShower_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_FSRPartonShower_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_FSRPartonShower_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_FSRPartonShower_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_FSRPartonShower_down', 'TT_FSRPartonShower_down', 'ST_FSRPartonShower_down', 'W+Jets_FSRPartonShower_down', 'Z+Jets_FSRPartonShower_down', 'HHbbVV_L1EcalPrefiring_down', 'ggHH_kl_2p45_kt_1_HHbbVV_L1EcalPrefiring_down', 'ggHH_kl_5_kt_1_HHbbVV_L1EcalPrefiring_down', 'ggHH_kl_0_kt_1_HHbbVV_L1EcalPrefiring_down', 'VBFHHbbVV_L1EcalPrefiring_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_L1EcalPrefiring_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_L1EcalPrefiring_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_L1EcalPrefiring_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_L1EcalPrefiring_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_L1EcalPrefiring_down', 'TT_L1EcalPrefiring_down', 'ST_L1EcalPrefiring_down', 'W+Jets_L1EcalPrefiring_down', 'Z+Jets_L1EcalPrefiring_down', 'HHbbVV_electron_id_down', 'ggHH_kl_2p45_kt_1_HHbbVV_electron_id_down', 'ggHH_kl_5_kt_1_HHbbVV_electron_id_down', 'ggHH_kl_0_kt_1_HHbbVV_electron_id_down', 'VBFHHbbVV_electron_id_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_electron_id_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_electron_id_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_electron_id_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_electron_id_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_electron_id_down', 'TT_electron_id_down', 'ST_electron_id_down', 'W+Jets_electron_id_down', 'Z+Jets_electron_id_down', 'HHbbVV_muon_id_down', 'ggHH_kl_2p45_kt_1_HHbbVV_muon_id_down', 'ggHH_kl_5_kt_1_HHbbVV_muon_id_down', 'ggHH_kl_0_kt_1_HHbbVV_muon_id_down', 'VBFHHbbVV_muon_id_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_muon_id_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_muon_id_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_muon_id_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_muon_id_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_muon_id_down', 'TT_muon_id_down', 'ST_muon_id_down', 'W+Jets_muon_id_down', 'Z+Jets_muon_id_down', 'HHbbVV_scale_down', 'ggHH_kl_2p45_kt_1_HHbbVV_scale_down', 'ggHH_kl_5_kt_1_HHbbVV_scale_down', 'ggHH_kl_0_kt_1_HHbbVV_scale_down', 'VBFHHbbVV_scale_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_scale_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_scale_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_scale_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_scale_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_scale_down', 'TT_scale_down', 'HHbbVV_pdf_down', 'ggHH_kl_2p45_kt_1_HHbbVV_pdf_down', 'ggHH_kl_5_kt_1_HHbbVV_pdf_down', 'ggHH_kl_0_kt_1_HHbbVV_pdf_down', 'VBFHHbbVV_pdf_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_pdf_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_pdf_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_pdf_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_pdf_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_pdf_down', 'HHbbVV_txbb_up', 'ggHH_kl_2p45_kt_1_HHbbVV_txbb_up', 'ggHH_kl_5_kt_1_HHbbVV_txbb_up', 'ggHH_kl_0_kt_1_HHbbVV_txbb_up', 'VBFHHbbVV_txbb_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_txbb_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_txbb_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_txbb_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_txbb_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_txbb_up', 'HHbbVV_pileup_up', 'ggHH_kl_2p45_kt_1_HHbbVV_pileup_up', 'ggHH_kl_5_kt_1_HHbbVV_pileup_up', 'ggHH_kl_0_kt_1_HHbbVV_pileup_up', 'VBFHHbbVV_pileup_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_pileup_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_pileup_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_pileup_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_pileup_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_pileup_up', 'TT_pileup_up', 'ST_pileup_up', 'W+Jets_pileup_up', 'Z+Jets_pileup_up', 'HHbbVV_pileupID_up', 'ggHH_kl_2p45_kt_1_HHbbVV_pileupID_up', 'ggHH_kl_5_kt_1_HHbbVV_pileupID_up', 'ggHH_kl_0_kt_1_HHbbVV_pileupID_up', 'VBFHHbbVV_pileupID_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_pileupID_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_pileupID_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_pileupID_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_pileupID_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_pileupID_up', 'TT_pileupID_up', 'ST_pileupID_up', 'W+Jets_pileupID_up', 'Z+Jets_pileupID_up', 'HHbbVV_ISRPartonShower_up', 'ggHH_kl_2p45_kt_1_HHbbVV_ISRPartonShower_up', 'ggHH_kl_5_kt_1_HHbbVV_ISRPartonShower_up', 'ggHH_kl_0_kt_1_HHbbVV_ISRPartonShower_up', 'VBFHHbbVV_ISRPartonShower_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_ISRPartonShower_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_ISRPartonShower_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_ISRPartonShower_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_ISRPartonShower_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_ISRPartonShower_up', 'TT_ISRPartonShower_up', 'ST_ISRPartonShower_up', 'W+Jets_ISRPartonShower_up', 'Z+Jets_ISRPartonShower_up', 'HHbbVV_FSRPartonShower_up', 'ggHH_kl_2p45_kt_1_HHbbVV_FSRPartonShower_up', 'ggHH_kl_5_kt_1_HHbbVV_FSRPartonShower_up', 'ggHH_kl_0_kt_1_HHbbVV_FSRPartonShower_up', 'VBFHHbbVV_FSRPartonShower_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_FSRPartonShower_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_FSRPartonShower_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_FSRPartonShower_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_FSRPartonShower_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_FSRPartonShower_up', 'TT_FSRPartonShower_up', 'ST_FSRPartonShower_up', 'W+Jets_FSRPartonShower_up', 'Z+Jets_FSRPartonShower_up', 'HHbbVV_L1EcalPrefiring_up', 'ggHH_kl_2p45_kt_1_HHbbVV_L1EcalPrefiring_up', 'ggHH_kl_5_kt_1_HHbbVV_L1EcalPrefiring_up', 'ggHH_kl_0_kt_1_HHbbVV_L1EcalPrefiring_up', 'VBFHHbbVV_L1EcalPrefiring_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_L1EcalPrefiring_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_L1EcalPrefiring_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_L1EcalPrefiring_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_L1EcalPrefiring_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_L1EcalPrefiring_up', 'TT_L1EcalPrefiring_up', 'ST_L1EcalPrefiring_up', 'W+Jets_L1EcalPrefiring_up', 'Z+Jets_L1EcalPrefiring_up', 'HHbbVV_electron_id_up', 'ggHH_kl_2p45_kt_1_HHbbVV_electron_id_up', 'ggHH_kl_5_kt_1_HHbbVV_electron_id_up', 'ggHH_kl_0_kt_1_HHbbVV_electron_id_up', 'VBFHHbbVV_electron_id_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_electron_id_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_electron_id_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_electron_id_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_electron_id_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_electron_id_up', 'TT_electron_id_up', 'ST_electron_id_up', 'W+Jets_electron_id_up', 'Z+Jets_electron_id_up', 'HHbbVV_muon_id_up', 'ggHH_kl_2p45_kt_1_HHbbVV_muon_id_up', 'ggHH_kl_5_kt_1_HHbbVV_muon_id_up', 'ggHH_kl_0_kt_1_HHbbVV_muon_id_up', 'VBFHHbbVV_muon_id_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_muon_id_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_muon_id_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_muon_id_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_muon_id_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_muon_id_up', 'TT_muon_id_up', 'ST_muon_id_up', 'W+Jets_muon_id_up', 'Z+Jets_muon_id_up', 'HHbbVV_scale_up', 'ggHH_kl_2p45_kt_1_HHbbVV_scale_up', 'ggHH_kl_5_kt_1_HHbbVV_scale_up', 'ggHH_kl_0_kt_1_HHbbVV_scale_up', 'VBFHHbbVV_scale_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_scale_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_scale_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_scale_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_scale_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_scale_up', 'TT_scale_up', 'HHbbVV_pdf_up', 'ggHH_kl_2p45_kt_1_HHbbVV_pdf_up', 'ggHH_kl_5_kt_1_HHbbVV_pdf_up', 'ggHH_kl_0_kt_1_HHbbVV_pdf_up', 'VBFHHbbVV_pdf_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_pdf_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_pdf_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_pdf_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_pdf_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_pdf_up'], name='Sample'),\n",
    "  Regular(20, 50, 250, name='bbFatJetParticleNetMass', label='$m^{bb}_\\\\mathrm{Reg}$ (GeV)'),\n",
    "  storage=Weight()) # Sum: WeightedSum(value=-nan, variance=88.7794) (WeightedSum(value=-nan, variance=88.7794) with flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2[\"pass\"][\"ST\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2[\"pass_JES_down\"][\"ST\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2[\"pass\"][\"HHbbVV_pileup_up\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2[\"pass\"][\"HHbbVV_pileup_up\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2[\"pass\"][\"HHbbVV\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2[\"passBlinded\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test how to load and use the *.pkl template file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_template(h, sample):\n",
    "    ''' \n",
    "    histogram h Hist, with axes:[\"samples\",\"systematic\",\"MH_Reco\"]\n",
    "    sample is sample name in [\"QCD\",...,\"data\"]\n",
    "    '''\n",
    "    mass_axis = 1 #axis index\n",
    "    massbins = h.axes[mass_axis].edges\n",
    "    return (h[sample, :].values(), h[sample, :].variances(), massbins, \"MH_Reco\")\n",
    "\n",
    "a = get_template(hists_template1[\"SR1a\"],\"QCD\")\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make some plots to test the variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins, x_min, x_max = 20, 50, 250\n",
    "\n",
    "plt.rcParams['axes.prop_cycle'] = cycler(color=[\"tab:blue\",\t\"tab:orange\",\t\"tab:green\",\t\"tab:red\",\t\"tab:purple\", \"tab:brown\", \"tab:pink\", \"k\",\"tab:olive\" ,\t\"tab:cyan\"])\n",
    "\n",
    "f = plt.figure(figsize=(14, 15))\n",
    "ax = f.add_subplot(1, 1, 1)  \n",
    "ax.grid()\n",
    "\n",
    "year = \"2018\"\n",
    "LUMI = {\"2016APV\": 36.33,\"2016\": 36.33, \"2017\": 41.48, \"2018\": 59.83,\"Full-Run2\":138}\n",
    "hep.cms.label(loc = 1, data=True, year=year, ax=ax, lumi=LUMI[year], fontsize=18, llabel='Preliminary')\n",
    "\n",
    "hist_region = bh.Histogram(bh.axis.Regular(nbins, x_min, x_max), storage=bh.storage.Weight())\n",
    "hep.histplot(get_template(hists_template1[\"CR1_pdfscale_up\"],\"TT\")[0], bins=get_template(hists_template1[\"SR1a_JES_up\"],\"TT\")[2], label=\"Test1 \", histtype='step', stack=False, linewidth=2, ax=ax,color = \"red\")\n",
    "\n",
    "hist_region = bh.Histogram(bh.axis.Regular(nbins, x_min, x_max), storage=bh.storage.Weight())\n",
    "hep.histplot(get_template(hists_template1[\"CR1\"],\"TT\")[0], bins=get_template(hists_template1[\"SR1a_JES_up\"],\"TT\")[2], label=\"Test2 \", histtype='step', stack=False, linewidth=2, ax=ax,color = \"blue\")\n",
    "\n",
    "hist_region = bh.Histogram(bh.axis.Regular(nbins, x_min, x_max), storage=bh.storage.Weight())\n",
    "hep.histplot(get_template(hists_template1[\"CR1_pdfscale_down\"],\"TT\")[0], bins=get_template(hists_template1[\"SR1a_JES_up\"],\"TT\")[2], label=\"Test3 \", histtype='step', stack=False, linewidth=2, ax=ax,color = \"orange\")\n",
    "\n",
    "\n",
    "ax.set_ylabel(\"Events\")\n",
    "ax.legend(loc=\"upper right\", ncol=1, frameon=False, fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make plots for PDF weight more professionally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = \".\"\n",
    "\n",
    "plot_dir_pro = f\"{MAIN_DIR}/plots/variation/8May24\"\n",
    "_ = os.system(f\"mkdir -p {plot_dir_pro}\")\n",
    "\n",
    "region = \"SR1a\"\n",
    "files_str = \"Rest\"\n",
    "year = YEAR\n",
    "bins = get_template(hists_template1[f\"{region}\"],\"WJets\")[2]\n",
    "\n",
    "plt.rcParams['axes.prop_cycle'] = cycler(color=[\"tab:blue\",\t\"tab:orange\",\t\"tab:green\",\t\"tab:red\",\t\"tab:purple\", \"tab:brown\", \"tab:pink\", \"k\",\"tab:olive\" ,\t\"tab:cyan\"])\n",
    "# plt.figure(figsize=(14,10))\n",
    "f = plt.figure(figsize=(14, 15))\n",
    "gs = mpl.gridspec.GridSpec(2, 1, height_ratios=[3, 1], hspace=0.08)\n",
    "ax = f.add_subplot(gs[0])\n",
    "plt.grid()\n",
    "LUMI = {\"2016\": 36.33, \"2017\": 41.48, \"2018\": 59.83,\"Full-Run2\":138}\n",
    "hep.cms.label(loc = 1, data=True, year=year, ax=ax, lumi=LUMI[year], fontsize=18, llabel='Preliminary')\n",
    "ax1 = f.add_subplot(gs[1])\n",
    "ax1.grid()\n",
    "\n",
    "hist_value_up = get_template(hists_template1[f\"{region}_pdfscale_up\"],files_str)[0]\n",
    "hist_var_up   = get_template(hists_template1[f\"{region}_pdfscale_up\"],files_str)[1]\n",
    "err_up = np.nan_to_num(error_bar(hist_value_up, hist_var_up, type = \"mc\"), nan = 0)\n",
    "hep.histplot(hist_value_up, bins=bins, yerr=err_up, label=\"PDF scale up\", histtype='step', stack=False, linewidth=2, ax=ax,color = \"red\")\n",
    "\n",
    "\n",
    "hist_value_nom = get_template(hists_template1[f\"{region}\"],files_str)[0]\n",
    "hist_var_nom   = get_template(hists_template1[f\"{region}\"],files_str)[1]\n",
    "err_nom = np.nan_to_num(error_bar(hist_value_nom, hist_var_nom, type = \"mc\"), nan = 0)\n",
    "hep.histplot(hist_value_nom, bins=bins, yerr=err_nom, label=\"center\", histtype='step', stack=False, linewidth=2, ax=ax,color = \"black\")\n",
    "\n",
    "\n",
    "hist_value_down = get_template(hists_template1[f\"{region}_pdfscale_down\"],files_str)[0]\n",
    "hist_var_down   = get_template(hists_template1[f\"{region}_pdfscale_down\"],files_str)[1]\n",
    "err_down = np.nan_to_num(error_bar(hist_value_down, hist_var_down, type = \"mc\"), nan = 0)\n",
    "hep.histplot(hist_value_down, bins=bins, yerr=err_down, label=\"PDF scale down\", histtype='step', stack=False, linewidth=2, ax=ax,color = \"blue\")\n",
    "\n",
    "err_up   = np.sqrt(np.power(err_up/hist_value_up,2) + np.power(err_nom/hist_value_nom,2))\n",
    "hep.histplot(hist_value_up/hist_value_nom,   bins=bins, yerr=err_up, color='red',  label=\"PDF scale up/center\", histtype='step', density=False, stack=False, ax=ax1,linewidth=2)\n",
    "err_down = np.sqrt(np.power(err_down/hist_value_down,2) + np.power(err_nom/hist_value_nom,2))    \n",
    "hep.histplot(hist_value_down/hist_value_nom, bins=bins, yerr=err_down, color='blue', label=\"PDF scale down/center\",histtype='step', density=False, stack=False, ax=ax1,linewidth=2)    \n",
    "\n",
    "\n",
    "ax1.set_xlabel(\"Higgs candidate MET recovery jet mass(GeV)\")\n",
    "ax.set_ylabel(\"Events\")\n",
    "ax1.set_ylabel(\"ratio\")\n",
    "ax1.set_ylim(0, 2)\n",
    "# ax.set_yscale('log') \n",
    "ax.legend(loc=\"upper right\", ncol=1, frameon=False, fontsize=22)\n",
    "ax1.legend(loc=\"upper right\", ncol=1, frameon=False, fontsize=20)\n",
    "plt.text(0.05,0.83,region+ \", \" + files_str,fontsize=24, color=\"black\", ha='left',transform=ax.transAxes)\n",
    "plt.savefig(f\"{plot_dir_pro}/pdf_{year}_{region}{files_str}.pdf\", bbox_inches='tight')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some other test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = {\n",
    "        \"CR1\" :{\"SRa\": \"SR1a\",\"SRb\":\"SR1b\"},\n",
    "        \"CR2\" :{\"SRa\": \"SR2a\",\"SRb\":\"SR2b\"},\n",
    "        \"CR3\" :{\"SRa\": \"SR3a\",\"SRb\":\"SR3b\"},\n",
    "        }\n",
    "\n",
    "regions_blinded = { key_fail + \"_blinded\": {key_pass + \"_blinded\" : key_pass_ab + \"_blinded\" for key_pass , key_pass_ab in key_pass_dict.items()}  for key_fail , key_pass_dict in regions.items()}\n",
    "regions_blinded.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"SR1a_blinded\"\n",
    "pass_region = (\"a_\" in region)\n",
    "pass_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"SR1aBlinded\"\n",
    "region_noblinded = region.split(\"Blinded\")[0]\n",
    "region_noblinded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for files in os.listdir(\"./root/\"):\n",
    "    if not files.startswith(\"2017\"):\n",
    "        os.system(f\"mv ./root/{files} ./root/2018_{files}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc8653c37afde981a02f518cc5ed66e36d68f5e1c41895fdf66da08341e86c45"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
